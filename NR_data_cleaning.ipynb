{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset 1 Hotels / campings / autres\n",
    "nr_import_a = pd.read_excel(\"20241024-Export-hotels--campings--aires-et-residences.xlsx\")\n",
    "\n",
    "#drop specific columns to adhere to import_2 column structure\n",
    "nr_import_a = nr_import_a.drop(columns=[\"Nombre de chambres déclarées\", \"Nombre d'appartement\", \"Emplacements nus\", \"Mobil-Home\" , \"Bungalow\",\"Chalet\",\"Emplacements déclarés\" , \"Emplacements Locatifs\", \"Emplacements résidentiels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset 2 locations\n",
    "nr_import_b = pd.read_excel(\"20241024-Export-locations.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate nr_import_a and nr_import_b vertically\n",
    "nr_import = pd.concat([nr_import_a, nr_import_b], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows where \"nom de l'offre\" is NaN and storing in seperate file\n",
    "\n",
    "# Separate rows with NaN in \"Nom de l'offre\" into a new DataFrame\n",
    "nr_name_nan = nr_import[nr_import[\"Nom de l'offre\"].isna()]\n",
    "\n",
    "# Drop rows with NaN in \"Nom de l'offre\" from the original DataFrame\n",
    "nr_import_2 = nr_import.dropna(subset=[\"Nom de l'offre\"])\n",
    "\n",
    "# Save the separate DataFrame with missing \"Nom de l'offre\" to an Excel file\n",
    "nr_name_nan.to_excel(\"nr_name_nan.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flagging potential duplicates in original df and create an export file for duplicates review\n",
    "\n",
    "# Create nr_import_3 as a copy of nr_import_2\n",
    "nr_import_3 = nr_import_2.copy()\n",
    "\n",
    "# Step 1: Mark potential duplicates in a new column \"duplicate\" in nr_import_3\n",
    "nr_import_3['Duplicate'] = nr_import_3.duplicated(subset=[\"Nom de l'offre\", \"Adresse\"], keep='first')\n",
    "\n",
    "# Step 2: Create a DataFrame containing all duplicates, including the first occurrence, for review\n",
    "nr_duplicates_for_review = nr_import_3[nr_import_3['Duplicate'] | nr_import_3.duplicated(subset=[\"Nom de l'offre\", \"Adresse\"], keep=False)]\n",
    "\n",
    "# Sort duplicates_for_review by \"Nom de l'offre\" in ascending order\n",
    "nr_duplicates_for_review = nr_duplicates_for_review.sort_values(by=\"Nom de l'offre\")\n",
    "\n",
    "# Export the duplicated lines (including first occurrences) to an Excel file\n",
    "nr_duplicates_for_review.to_excel(\"nr_duplicated_rows_review.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping & renaming columns / data casting\n",
    "\n",
    "#dropping  \n",
    "nr_import_4 = nr_import_3.drop(columns=[\"Label Tourisme et handicap\", \"Modes de paiement acceptés\", \"Groupes acceptés\", \"Types de groupes acceptés\",\"Tarifs\"])\n",
    "\n",
    "#renaming columns\n",
    "nr_import_5 = nr_import_4.rename(columns={\"Nom de l'offre\": \"Offre\", \"Code postal\" : \"Code_postal\", \"Animaux acceptés\":\"Animaux_acceptes\", \"Catégorie\":\"Categorie\", \"Capacité totale\":\"Capacite\",\"Services et équipements\":\"Services\"})\n",
    "\n",
    "#Data type casting Code_postal & INSEE to object\n",
    "# First, cast to integer to remove the decimal, then to string for final output\n",
    "nr_import_5[\"Code_postal\"] = nr_import_5[\"Code_postal\"].fillna(0).astype(int).astype(str)\n",
    "nr_import_5[\"INSEE\"] = nr_import_5[\"INSEE\"].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treating label column / extracting list / create column label_count\n",
    "\n",
    "# Replace \"Vélo\" with \"vélo\" in the 'Label' column\n",
    "nr_import_5['Label'] = nr_import_5['Label'].str.replace(r'\\bVélo\\b', 'vélo', regex=True)\n",
    "\n",
    "# Drop NaN values in the \"Label\" column and ensure all values are treated as strings\n",
    "all_labels = nr_import_5['Label'].dropna().astype(str)\n",
    "\n",
    "# Split each cell by commas, expand the results, strip spaces, and get unique values\n",
    "all_labels = all_labels.str.split(',').explode().str.strip()\n",
    "\n",
    "# Get unique labels, converting to a sorted list\n",
    "unique_labels = sorted(all_labels.unique())\n",
    "\n",
    "# Create a DataFrame from the sorted list and add an index column\n",
    "labels_list_db = pd.DataFrame(unique_labels, columns=[\"Label\"]).sort_values(by=\"Label\").reset_index()\n",
    "labels_list_db.rename(columns={\"index\": \"Label_ID\"}, inplace=True)  # Renaming the index column if desired\n",
    "\n",
    "# Export to Excel, with the Label_ID as a column in the sheet\n",
    "labels_list_db.to_excel(\"labels_list_db.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column \"label_count\" that counts the number of different labels in the \"Label\" column\n",
    "# Split each entry by commas, count the parts, and handle NaN values by filling with 0\n",
    "nr_import_5['Label_count'] = nr_import_5['Label'].fillna('').apply(lambda x: len(x.split(',')) if x else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying opening scheme with regex\n",
    "\n",
    "import re\n",
    "\n",
    "# Create nr_import_6 as a copy of nr_import_5\n",
    "nr_import_6 = nr_import_5.copy()\n",
    "\n",
    "# Define the function to determine the opening scheme\n",
    "def determine_opening_scheme(row):\n",
    "    # Step 1: Check \"Période ouverture et fermeture\" columns first\n",
    "    periode_text = \" \".join(str(row[col]) for col in [\n",
    "        \"Période ouverture et fermeture 1\", \"Période ouverture et fermeture 2\", \"Période ouverture et fermeture 3\"\n",
    "    ] if pd.notna(row[col])).strip()\n",
    "\n",
    "    # Check for all-year patterns in \"Période ouverture et fermeture\" columns\n",
    "    if re.search(r\"toute l'année|all year|année complète|year round\", periode_text, re.IGNORECASE):\n",
    "        return \"Annee\"\n",
    "    elif re.search(r\"janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre|hiver|été|printemps|automne|saison\", periode_text, re.IGNORECASE):\n",
    "        return \"Saison\"\n",
    "    \n",
    "    # Step 2: If no result from \"Période ouverture et fermeture\" columns, check \"Ouverture accueil (texte)\"\n",
    "    accueil_text = str(row[\"Ouverture accueil (texte)\"]) if pd.notna(row[\"Ouverture accueil (texte)\"]) else \"\"\n",
    "\n",
    "    if re.search(r\"toute l'année|all year|année complète|year round\", accueil_text, re.IGNORECASE):\n",
    "        return \"Annee\"\n",
    "    elif re.search(r\"janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre|hiver|été|printemps|automne|saison\", accueil_text, re.IGNORECASE):\n",
    "        return \"Saison\"\n",
    "    \n",
    "    # Default to blank if no pattern is detected\n",
    "    return \"\"\n",
    "\n",
    "# Apply the function to each row to create the \"Ouverture\" column in nr_import_6\n",
    "nr_import_6['Ouverture'] = nr_import_6.apply(determine_opening_scheme, axis=1)\n",
    "\n",
    "# Drop the columns that are no longer needed\n",
    "nr_import_6 = nr_import_6.drop(columns=[\"Ouverture accueil (texte)\", \"Période ouverture et fermeture 1\", \"Période ouverture et fermeture 2\",\"Période ouverture et fermeture 3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract unique values from the 'Type' column\n",
    "unique_types = nr_import_6['Type'].unique()\n",
    "\n",
    "# Step 2: Convert the unique values into a DataFrame\n",
    "type_offre_db = pd.DataFrame(unique_types, columns=['Type_offre'])\n",
    "\n",
    "# Step 3: Sort values alphabetically, treating blanks as '0' and assigning an index\n",
    "type_offre_db = type_offre_db.sort_values(by='Type_offre', key=lambda x: x.replace('', '0')).reset_index(drop=True)\n",
    "\n",
    "# Step 4: Assign a custom index, with blank types as index 0\n",
    "type_offre_db['type_offre_ID'] = range(1, len(type_offre_db) + 1)\n",
    "type_offre_db.loc[type_offre_db['Type_offre'] == '', 'type_offre_ID'] = 0\n",
    "\n",
    "# Step 5: Reorder columns so 'Index' is the first column and sort by 'Index'\n",
    "type_offre_db = type_offre_db[['type_offre_ID', 'Type_offre']].sort_values(by='type_offre_ID').reset_index(drop=True)\n",
    "\n",
    "#export to XL\n",
    "type_offre_db.to_excel(\"type_offre_db.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All TSID values are unique.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the TSID column\n",
    "duplicate_tsid = nr_import_6['TSID'].duplicated().any()\n",
    "\n",
    "if duplicate_tsid:\n",
    "    print(\"There are duplicate TSID values.\")\n",
    "else:\n",
    "    print(\"All TSID values are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a copy of nr_import_6\n",
    "nr_import_7 = nr_import_6.copy()\n",
    "\n",
    "# Step 2: Merge nr_import_7 with type_offre on the 'Type' column to get the corresponding IDs\n",
    "nr_import_7 = nr_import_7.merge(type_offre_db, left_on='Type', right_on='Type_offre', how='left')\n",
    "\n",
    "# Step 3: Rename the 'Index' column from type_offre to 'Type_ID' for clarity\n",
    "nr_import_7 = nr_import_7.rename(columns={'Index': 'Type_offre_ID'})\n",
    "\n",
    "# Step 4: Drop the 'Type_offre' column as it's no longer needed after the merge\n",
    "nr_import_7 = nr_import_7.drop(columns=['Type_offre'])\n",
    "\n",
    "#export to XL\n",
    "nr_import_7.to_excel(\"backup_offer_all.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sub dataset with Service and description for future NLP\n",
    "\n",
    "# Select the desired columns from nr_import_7 and create descriptis_db\n",
    "nr_descriptif = nr_import_7[['TSID', 'Offre', 'Services', 'Descriptif']].copy()\n",
    "\n",
    "#export to XL\n",
    "nr_descriptif.to_excel(\"descriptif_db.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of nr_import_7 and drop the specified columns\n",
    "offres_normandie_db = nr_import_7.drop(columns=[\"Type\", \"Descriptif\"]).copy()\n",
    "\n",
    "#export to XL\n",
    "offres_normandie_db.to_excel(\"offres_normandie_db.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
